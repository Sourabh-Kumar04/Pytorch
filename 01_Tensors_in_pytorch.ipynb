{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b190176c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0+cu124\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b4d5a36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU is not available\n",
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"GPU is available\")\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"Number of GPYUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "else:\n",
    "    print(\"GPU is not available\")\n",
    "    print(\"Using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11542d2e",
   "metadata": {},
   "source": [
    "# Creating a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f167772",
   "metadata": {},
   "source": [
    "### 1. Using Empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a149704",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.6020e-11, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 4.4842e-44, 0.0000e+00]])\n",
      "Type of a: torch.float32\n",
      "\n",
      "tensor([[5.5858e-14, 0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 5.5607e-14, 0.0000e+00]])\n",
      "Type of b: torch.float32\n",
      "\n",
      "tensor([[708864014,         0,         0],\n",
      "        [        0,         0,         0]], dtype=torch.int32)\n",
      "Type of b: torch.int32\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using Empty\n",
    "# torch.empty() creates a tensor with uninitialized data\n",
    "\n",
    "# torch.empty(2, 3)\n",
    "a = torch.empty(2, 3)\n",
    "print(a)\n",
    "type(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "\n",
    "# torch.empty(2, 3, dtype=torch.float64)\n",
    "b = torch.empty(2, 3)\n",
    "print(b)\n",
    "type(b)\n",
    "print(f\"Type of b: {b.dtype}\")\n",
    "print()\n",
    "\n",
    "# torch.empty(2, 3, dtype=torch.int32)\n",
    "c = torch.empty(2, 3, dtype=torch.int32)\n",
    "print(c)\n",
    "type(c)\n",
    "print(f\"Type of b: {c.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b796ace",
   "metadata": {},
   "source": [
    "### 2. Using Zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9540a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Type of a: torch.float32\n",
      "\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "Type of a: torch.float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "print(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "b = torch.zeros(2, 3, dtype=torch.float64)\n",
    "print(b)\n",
    "print(f\"Type of a: {b.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde5a942",
   "metadata": {},
   "source": [
    "### 3. Using ones\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "476ed211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "Type of a: torch.float32\n",
      "\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n",
      "Type of a: torch.float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "b = torch.ones(2, 3, dtype=torch.float64)\n",
    "print(b)\n",
    "print(f\"Type of a: {b.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a15e71",
   "metadata": {},
   "source": [
    "### 4. Using rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8818c148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6598, 0.8759, 0.2131],\n",
      "        [0.6149, 0.0900, 0.7302]])\n",
      "Type of a: torch.float32\n",
      "\n",
      "tensor([[0.4197, 0.2955, 0.4449],\n",
      "        [0.6132, 0.0473, 0.8456]], dtype=torch.float64)\n",
      "Type of a: torch.float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Always use torch.rand() to create a tensor with random values\n",
    "# torch.rand() creates a tensor with random values\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "b = torch.rand(2, 3, dtype=torch.float64)\n",
    "print(b)\n",
    "print(f\"Type of a: {b.dtype}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30ac5b1",
   "metadata": {},
   "source": [
    "### 5. manual_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32788eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1117, 0.8158, 0.2626],\n",
      "        [0.4839, 0.6765, 0.7539]])\n",
      "Type of a: torch.float32\n",
      "\n",
      "tensor([[0.1015, 0.6642, 0.9736],\n",
      "        [0.6941, 0.3464, 0.9751]], dtype=torch.float64)\n",
      "Type of a: torch.float64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Always use torch.manual_seed() to set the random seed\n",
    "# torch.manual_seed(0) sets the random seed\n",
    "\n",
    "torch.manual_seed(100)\n",
    "\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "\n",
    "b = torch.rand(2, 3, dtype=torch.float64)\n",
    "print(b)\n",
    "print(f\"Type of a: {b.dtype}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4087ae63",
   "metadata": {},
   "source": [
    "### 6. Using arange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ead000c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "Type of a: torch.int64\n",
      "\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# torch.arange() creates a 1D tensor with values from start to end\n",
    "a = torch.arange(1, 10)\n",
    "print(a)\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()\n",
    "\n",
    "#  2d tensor\n",
    "b = torch.arange(1, 10).reshape(3, 3)\n",
    "print(b)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f25ae6",
   "metadata": {},
   "source": [
    "### 7. Using tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1638e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "\n",
      "Type of a: torch.int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a)\n",
    "print()\n",
    "print(f\"Type of a: {a.dtype}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9252638f",
   "metadata": {},
   "source": [
    "### 8. Other ways\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1606f30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using linspacec:  tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])\n",
      "Using linspace:  tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])\n",
      "\n",
      "Usinf logspace:  tensor([1.0000e+01, 1.7783e+03, 3.1623e+05, 5.6234e+07, 1.0000e+10])\n",
      "\n",
      "Using eye:  tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 1.]])\n",
      "\n",
      "Using full:  tensor([[5, 5, 5],\n",
      "        [5, 5, 5]])\n",
      "Using full_like:  tensor([5., 5., 5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "# using linspace\n",
    "a = torch.linspace(1, 10, steps=5)\n",
    "print(\"Using linspacec: \", a)\n",
    "\n",
    "b = torch.linspace(1, 10, 5)\n",
    "print(\"Using linspace: \" ,b)\n",
    "\n",
    "print()\n",
    "\n",
    "# using logspace\n",
    "c = torch.logspace(1, 10, 5)\n",
    "print(\"Usinf logspace: \",c)\n",
    "print()\n",
    "\n",
    "# using eye\n",
    "d = torch.eye(5)\n",
    "print(\"Using eye: \", d)\n",
    "print()\n",
    "\n",
    "# using full\n",
    "e = torch.full((2, 3), 5)\n",
    "print(\"Using full: \", e)\n",
    "\n",
    "# using full_like\n",
    "f = torch.full_like(a, 5)\n",
    "print(\"Using full_like: \", f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf48309",
   "metadata": {},
   "source": [
    "# Tensor Shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dbc24e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ff1c647",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[                  0, 7310593858020254331, 3616445622929465956],\n",
       "        [6066966817751969077, 3977868362401855793, 6500725275829417006]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.empty_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "868bd437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0],\n",
       "        [0, 0, 0]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.zeros_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1],\n",
       "        [1, 1, 1]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ones_like(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71d19045",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7911, 0.4274, 0.4460],\n",
       "        [0.5522, 0.9559, 0.9405]], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.rand_like(x)  ##RuntimeError: \"check_uniform_bounds\" not implemented for 'Long'\n",
    "\n",
    "torch.rand_like(x, dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65a1c46",
   "metadata": {},
   "source": [
    "# Tensor Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "99e8a813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find the data type\n",
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6]], dtype=torch.int32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assign a data type\n",
    "torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b3a3df7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3.],\n",
       "        [4., 5., 6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  using to()(\n",
    "x.to(torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a4b97",
   "metadata": {},
   "source": [
    "| **Data Type**             | **Dtype**         | **Description**                                                                                                                                                                |\n",
    "|---------------------------|-------------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **32-bit Floating Point** | `torch.float32`   | Standard floating-point type used for most deep learning tasks. Provides a balance between precision and memory usage.                                                         |\n",
    "| **64-bit Floating Point** | `torch.float64`   | Double-precision floating point. Useful for high-precision numerical tasks but uses more memory.                                                                               |\n",
    "| **16-bit Floating Point** | `torch.float16`   | Half-precision floating point. Commonly used in mixed-precision training to reduce memory and computational overhead on modern GPUs.                                            |\n",
    "| **BFloat16**              | `torch.bfloat16`  | Brain floating-point format with reduced precision compared to `float16`. Used in mixed-precision training, especially on TPUs.                                                |\n",
    "| **8-bit Floating Point**  | `torch.float8`    | Ultra-low-precision floating point. Used for experimental applications and extreme memory-constrained environments (less common).                                               |\n",
    "| **8-bit Integer**         | `torch.int8`      | 8-bit signed integer. Used for quantized models to save memory and computation in inference.                                                                                   |\n",
    "| **16-bit Integer**        | `torch.int16`     | 16-bit signed integer. Useful for special numerical tasks requiring intermediate precision.                                                                                    |\n",
    "| **32-bit Integer**        | `torch.int32`     | Standard signed integer type. Commonly used for indexing and general-purpose numerical tasks.                                                                                  |\n",
    "| **64-bit Integer**        | `torch.int64`     | Long integer type. Often used for large indexing arrays or for tasks involving large numbers.                                                                                  |\n",
    "| **8-bit Unsigned Integer**| `torch.uint8`     | 8-bit unsigned integer. Commonly used for image data (e.g., pixel values between 0 and 255).                                                                                    |\n",
    "| **Boolean**               | `torch.bool`      | Boolean type, stores `True` or `False` values. Often used for masks in logical operations.                                                                                      |\n",
    "| **Complex 64**            | `torch.complex64` | Complex number type with 32-bit real and 32-bit imaginary parts. Used for scientific and signal processing tasks.                                                               |\n",
    "| **Complex 128**           | `torch.complex128`| Complex number type with 64-bit real and 64-bit imaginary parts. Offers higher precision but uses more memory.                                                                 |\n",
    "| **Quantized Integer**     | `torch.qint8`     | Quantized signed 8-bit integer. Used in quantized models for efficient inference.                                                                                              |\n",
    "| **Quantized Unsigned Integer** | `torch.quint8` | Quantized unsigned 8-bit integer. Often used for quantized tensors in image-related tasks.                                                                                     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99a88f21",
   "metadata": {},
   "source": [
    "# Mathematical Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a046032",
   "metadata": {},
   "source": [
    "### 1.  Scalar Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c9824ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5277, 0.2472],\n",
       "        [0.7909, 0.4235]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.rand(2, 2)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition:  tensor([[2.5277, 2.2472],\n",
      "        [2.7909, 2.4235]])\n",
      "\n",
      "Subtraction:  tensor([[-1.4723, -1.7528],\n",
      "        [-1.2091, -1.5765]])\n",
      "\n",
      "Multiplication:  tensor([[1.0554, 0.4944],\n",
      "        [1.5818, 0.8470]])\n",
      "\n",
      "Division:  tensor([[0.1759, 0.0824],\n",
      "        [0.2636, 0.1412]])\n",
      "\n",
      "Integer Division:  tensor([[17.,  8.],\n",
      "        [26., 14.]])\n",
      "\n",
      "Modulus:  tensor([[1., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "Power:  tensor([[0.2785, 0.0611],\n",
      "        [0.6255, 0.1793]])\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "x + 2\n",
    "print(\"Addition: \", x + 2)\n",
    "print()\n",
    "\n",
    "# Subtraction\n",
    "x - 2\n",
    "print(\"Subtraction: \", x - 2)\n",
    "print()\n",
    "\n",
    "# Multiplication\n",
    "x * 2\n",
    "print(\"Multiplication: \", x * 2)\n",
    "print()\n",
    "\n",
    "# Division\n",
    "x / 3\n",
    "print(\"Division: \", x / 3)\n",
    "print()\n",
    "\n",
    "# Integer Division\n",
    "(x * 100) // 3\n",
    "print(\"Integer Division: \", (x * 100) // 3)\n",
    "print()\n",
    "\n",
    "# Modulus\n",
    "((x * 100) // 3) % 2\n",
    "print(\"Modulus: \", ((x * 100) // 3) % 2)\n",
    "print()\n",
    "\n",
    "# Power\n",
    "x ** 2\n",
    "print(\"Power: \", x ** 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd009282",
   "metadata": {},
   "source": [
    "### 2. Element wise Operations "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535ece76",
   "metadata": {},
   "source": [
    "# âš¡ Element-wise Operations in PyTorch (Research Guide)\n",
    "\n",
    "Element-wise operations apply a function to each individual element of one or more tensors. These operations are foundational in deep learning for vectorized computation.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Arithmetic Element-wise Operations\n",
    "\n",
    "| Operation      | PyTorch Function             | Example                           |\n",
    "|----------------|------------------------------|-----------------------------------|\n",
    "| Addition       | `torch.add`, `+`             | `a + b`                           |\n",
    "| Subtraction    | `torch.sub`, `-`             | `a - b`                           |\n",
    "| Multiplication | `torch.mul`, `*`             | `a * b`                           |\n",
    "| Division       | `torch.div`, `/`             | `a / b`                           |\n",
    "| Floor Division | `torch.floor_divide`, `//`   | `a // b`                          |\n",
    "| Modulus        | `torch.remainder`, `%`       | `a % b`                           |\n",
    "| Power          | `torch.pow`, `**`            | `a ** b`                          |\n",
    "\n",
    "```\n",
    "a = torch.tensor([1., 2., 3.])\n",
    "b = torch.tensor([4., 5., 6.])\n",
    "print(a + b)  # tensor([5., 7., 9.])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Comparison Element-wise Operations\n",
    "\n",
    "| Operation           | PyTorch Function       | Example             |\n",
    "|---------------------|------------------------|---------------------|\n",
    "| Equal               | `torch.eq`             | `a == b`            |\n",
    "| Not Equal           | `torch.ne`             | `a != b`            |\n",
    "| Greater Than        | `torch.gt`             | `a > b`             |\n",
    "| Greater or Equal    | `torch.ge`             | `a >= b`            |\n",
    "| Less Than           | `torch.lt`             | `a < b`             |\n",
    "| Less or Equal       | `torch.le`             | `a <= b`            |\n",
    "\n",
    "Returns a Boolean tensor (`torch.bool` dtype).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Logical Element-wise Operations\n",
    "\n",
    "| Operation      | PyTorch Function     | Description                        |\n",
    "|----------------|----------------------|------------------------------------|\n",
    "| AND            | `torch.logical_and`  | Element-wise logical AND           |\n",
    "| OR             | `torch.logical_or`   | Element-wise logical OR            |\n",
    "| XOR            | `torch.logical_xor`  | Element-wise logical XOR           |\n",
    "| NOT            | `torch.logical_not`  | Element-wise logical NOT           |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Trigonometric and Exponential Element-wise Ops\n",
    "\n",
    "| Function              | Description                        |\n",
    "|-----------------------|------------------------------------|\n",
    "| `torch.sin`, `cos`, `tan`       | Trigonometric functions           |\n",
    "| `torch.asin`, `acos`, `atan`    | Inverse trigonometric             |\n",
    "| `torch.exp`, `torch.log`        | Exponential and logarithm         |\n",
    "| `torch.sqrt`, `torch.rsqrt`     | Square root, reciprocal sqrt      |\n",
    "| `torch.abs`                     | Absolute value                    |\n",
    "| `torch.ceil`, `floor`, `round`  | Rounding functions                |\n",
    "| `torch.clamp(x, min, max)`      | Clamp values within range         |\n",
    "| `torch.sigmoid`, `tanh`, `relu` | Common activation functions       |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Bitwise Element-wise Operations (on integers)\n",
    "\n",
    "| Operation      | PyTorch Function     |\n",
    "|----------------|----------------------|\n",
    "| AND            | `torch.bitwise_and`  |\n",
    "| OR             | `torch.bitwise_or`   |\n",
    "| XOR            | `torch.bitwise_xor`  |\n",
    "| NOT            | `torch.bitwise_not`  |\n",
    "| Left Shift     | `torch.bitwise_left_shift` |\n",
    "| Right Shift    | `torch.bitwise_right_shift` |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. Broadcasting in Element-wise Ops\n",
    "\n",
    "PyTorch supports **broadcasting**, which allows element-wise ops between tensors of different shapes under certain conditions.\n",
    "\n",
    "```\n",
    "a = torch.tensor([[1], [2], [3]])  # Shape: [3, 1]\n",
    "b = torch.tensor([10, 20, 30])     # Shape: [3]\n",
    "c = a * b  # Broadcasting to shape [3, 3]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 7. In-place Element-wise Ops (Memory Efficient)\n",
    "\n",
    "Use `_` suffix to perform operation in-place.\n",
    "\n",
    "| Operation | In-place Function     |\n",
    "|-----------|------------------------|\n",
    "| Add       | `a.add_(b)`            |\n",
    "| Subtract  | `a.sub_(b)`            |\n",
    "| Multiply  | `a.mul_(b)`            |\n",
    "| Divide    | `a.div_(b)`            |\n",
    "\n",
    "> âš ï¸ Note: In-place ops can interfere with autograd if used improperly.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary Table\n",
    "\n",
    "| Category       | Common Ops                              |\n",
    "|----------------|------------------------------------------|\n",
    "| Arithmetic     | `+`, `-`, `*`, `/`, `**`, `torch.pow`    |\n",
    "| Comparison     | `torch.eq`, `ne`, `gt`, `lt`, etc.       |\n",
    "| Logical        | `logical_and`, `or`, `xor`, `not`        |\n",
    "| Trigonometric  | `sin`, `cos`, `tan`, `exp`, `log`, etc.  |\n",
    "| Bitwise        | `bitwise_and`, `or`, `not`, `xor`        |\n",
    "| In-place Ops   | `add_`, `mul_`, etc.                     |\n",
    "| Broadcasting   | Auto-expand for compatible shapes        |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… These operations are **highly optimized** and essential for **deep learning model development**, **data manipulation**, and **loss computations**.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "618b1439",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0169, 0.2209, 0.9535],\n",
      "        [0.7064, 0.1629, 0.8902]])\n",
      "\n",
      "tensor([[0.5163, 0.0359, 0.6476],\n",
      "        [0.3430, 0.3182, 0.5261]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(2, 3)\n",
    "b = torch.rand(2, 3)\n",
    "\n",
    "print(a)\n",
    "print()\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition of a and b is : tensor([[0.5332, 0.2568, 1.6012],\n",
      "        [1.0494, 0.4811, 1.4163]])\n",
      "\n",
      "Subtraction of a and b is : tensor([[-0.4994,  0.1850,  0.3059],\n",
      "        [ 0.3634, -0.1554,  0.3641]])\n",
      "\n",
      "Multiplication of a and b is : tensor([[0.0087, 0.0079, 0.6175],\n",
      "        [0.2423, 0.0518, 0.4683]])\n",
      "\n",
      "Division of a and b is : tensor([[0.0327, 6.1557, 1.4723],\n",
      "        [2.0593, 0.5118, 1.6921]])\n",
      "\n",
      "Power : tensor([[0.1216, 0.9473, 0.9697],\n",
      "        [0.8876, 0.5613, 0.9406]])\n",
      "\n",
      "Modulus of a and b is : tensor([[0.0169, 0.0056, 0.3059],\n",
      "        [0.0204, 0.1629, 0.3641]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Addition\n",
    "print(f\"Addition of a and b is : {a + b}\")\n",
    "print()\n",
    "\n",
    "# Subtraction\n",
    "print(f\"Subtraction of a and b is : {a - b}\")\n",
    "print()\n",
    "\n",
    "# Multiplication\n",
    "print(f\"Multiplication of a and b is : {a * b}\")\n",
    "print()\n",
    "\n",
    "# Division\n",
    "print(f\"Division of a and b is : {a / b}\")\n",
    "print()\n",
    "\n",
    "# Power\n",
    "print(f\"Power : {a ** b}\")\n",
    "print()\n",
    "\n",
    "# Modulus\n",
    "print(f\"Modulus of a and b is : {a % b}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e508df36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1, -2,  3, -4])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.tensor([1, -2, 3, -4])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c21e5a1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# abs\n",
    "torch.abs(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab0c8bea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1,  2, -3,  4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative\n",
    "torch.negative(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.9000, 2.3000, 3.7000, 4.4000])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.tensor([1.9, 2.3, 3.7, 4.4])\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2., 2., 4., 4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# round\n",
    "torch.round(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.3000, 3.7000, 4.4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2., 3., 4., 5.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ceil - greater integer\n",
    "print(d)\n",
    "torch.ceil(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.3000, 3.7000, 4.4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3., 4.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# floor - smaller integer\n",
    "print(d)\n",
    "torch.floor(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.9000, 2.3000, 3.7000, 4.4000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([2.0000, 2.3000, 3.0000, 3.0000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clamp - put numbers within a range\n",
    "# this is called clamping operation\n",
    "print(d)\n",
    "torch.clamp(d, min=2, max=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b8e675",
   "metadata": {},
   "source": [
    "### 3. Reduction Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8561021b",
   "metadata": {},
   "source": [
    "## ðŸ§  PyTorch Reduction Operations â€“ Complete Guide\n",
    "\n",
    "In PyTorch, **reduction operations** condense tensors along one or more dimensions into a **single value or lower-dimensional tensor**. These are often used in **loss computation**, **statistics**, **summaries**, and **aggregation in models**.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ Categories of Reduction Operations\n",
    "\n",
    "| Category             | Examples |\n",
    "|----------------------|----------|\n",
    "| **Summation-based**  | `sum`, `mean`, `prod`, `cumsum`, `cumprod` |\n",
    "| **Min/Max-based**    | `min`, `max`, `argmin`, `argmax`, `amin`, `amax` |\n",
    "| **Norm-based**       | `norm`, `frobenius_norm`, `nuclear_norm` |\n",
    "| **Statistical**      | `std`, `var`, `median`, `quantile`, `mode` |\n",
    "| **Logical**          | `all`, `any` |\n",
    "| **Others**           | `logsumexp`, `count_nonzero`, `unique`, `kthvalue`, `topk` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 1. **Summation-Based Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.sum(input, dim)` | Sum of elements across dimensions |\n",
    "| `torch.mean(input, dim)` | Mean of elements |\n",
    "| `torch.prod(input, dim)` | Product of elements |\n",
    "| `torch.cumsum(input, dim)` | Cumulative sum |\n",
    "| `torch.cumprod(input, dim)` | Cumulative product |\n",
    "\n",
    "ðŸ”¸ Example:\n",
    "```python\n",
    "x = torch.tensor([[1, 2], [3, 4]])\n",
    "torch.sum(x, dim=0)  # tensor([4, 6])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 2. **Min/Max-Based Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.min`, `torch.max` | Returns min/max (optionally along dim) |\n",
    "| `torch.amin`, `torch.amax` | Same as min/max but works element-wise |\n",
    "| `torch.argmin`, `torch.argmax` | Indices of min/max values |\n",
    "| `torch.clamp(input, min, max)` | Bounds values between min and max |\n",
    "| `torch.kthvalue(input, k, dim)` | k-th smallest value and index |\n",
    "| `torch.topk(input, k, dim)` | Top-k largest values and indices |\n",
    "\n",
    "ðŸ”¸ Example:\n",
    "```python\n",
    "x = torch.tensor([1, 3, 2])\n",
    "torch.topk(x, 2)  # values=tensor([3, 2]), indices=tensor([1, 2])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 3. **Norm-Based Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.norm(input, p='fro')` | Vector or matrix norm |\n",
    "| `torch.linalg.vector_norm`, `matrix_norm` | Explicit norm functions |\n",
    "| `torch.nn.functional.normalize()` | Normalizes tensor (unit norm) |\n",
    "\n",
    "ðŸ”¸ Example:\n",
    "```python\n",
    "torch.norm(torch.tensor([3.0, 4.0]))  # 5.0 (Euclidean norm)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 4. **Statistical Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.std(input, dim)` | Standard deviation |\n",
    "| `torch.var(input, dim)` | Variance |\n",
    "| `torch.median(input)` | Median value |\n",
    "| `torch.mode(input)` | Most frequent element |\n",
    "| `torch.quantile(input, q)` | q-th quantile value |\n",
    "\n",
    "ðŸ”¸ Example:\n",
    "```python\n",
    "x = torch.tensor([1., 2., 3., 4.])\n",
    "torch.quantile(x, 0.5)  # Median â†’ 2.5\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 5. **Logical Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.all(input, dim)` | True if all elements are true |\n",
    "| `torch.any(input, dim)` | True if any element is true |\n",
    "\n",
    "ðŸ”¸ Example:\n",
    "```\n",
    "x = torch.tensor([True, False])\n",
    "torch.all(x)  # False\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ 6. **Other Reductions**\n",
    "\n",
    "| Function | Description |\n",
    "|----------|-------------|\n",
    "| `torch.logsumexp(input, dim)` | Stable `log(sum(exp(x)))` â€“ useful in softmax/logits |\n",
    "| `torch.count_nonzero(input)` | Number of non-zero elements |\n",
    "| `torch.unique(input)` | Unique elements |\n",
    "| `torch.numel(input)` | Total number of elements in a tensor |\n",
    "| `torch.bincount(input)` | Histogram of counts for 1D int tensor |\n",
    "\n",
    "---\n",
    "\n",
    "### âœ… Choosing the Right Reduction\n",
    "\n",
    "| Task Type        | Recommended Reductions |\n",
    "|------------------|------------------------|\n",
    "| Classification   | `torch.mean`, `torch.argmax`, `torch.nn.CrossEntropyLoss` |\n",
    "| Statistical ML   | `torch.var`, `torch.std`, `torch.median` |\n",
    "| NLP (Logits)     | `torch.logsumexp`, `torch.softmax` |\n",
    "| Image Segmentation | `torch.count_nonzero`, `torch.sum` over masks |\n",
    "| Attention Mechanisms | `torch.topk`, `torch.softmax`, `torch.mean` |\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“Œ Summary Chart\n",
    "\n",
    "```\n",
    "+----------------+------------------------------------------+\n",
    "| Function       | Use Case                                |\n",
    "+----------------+------------------------------------------+\n",
    "| sum / mean     | General aggregation                      |\n",
    "| min / max      | Boundary values                          |\n",
    "| argmin / argmax| Label prediction                         |\n",
    "| std / var      | Spread of data                           |\n",
    "| norm           | Distance measures, regularization        |\n",
    "| logsumexp      | Numerical stability in log space         |\n",
    "| topk           | Ranking or attention scores              |\n",
    "| count_nonzero  | Mask operations                          |\n",
    "+----------------+------------------------------------------+\n",
    "```\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 9., 8.],\n",
       "        [9., 7., 9.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.randint(size=(2, 3), low=0, high=10, dtype=float)\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Sum of tensor is : 47.0\n",
      "\n",
      "Sum of tensor along columns is : tensor([14., 16., 17.], dtype=torch.float64)\n",
      "\n",
      "Sum of tensor along rows is : tensor([22., 25.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# sum\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Sum of tensor is : {torch.sum(e)}\")\n",
    "print()\n",
    "\n",
    "# sum along columns\n",
    "print(f\"Sum of tensor along columns is : {torch.sum(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# sum along rows\n",
    "print(f\"Sum of tensor along rows is : {torch.sum(e, dim=1)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Mean of tensor is : 7.833333333333333\n",
      "\n",
      "Mean of tensor along columns is : tensor([7.0000, 8.0000, 8.5000], dtype=torch.float64)\n",
      "\n",
      "Mean of tensor along rows is : tensor([7.3333, 8.3333], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# mean works only with floating point or complex dtype\n",
    "\n",
    "# mean\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Mean of tensor is : {torch.mean(e)}\")\n",
    "print()\n",
    "\n",
    "# mean along columns\n",
    "print(f\"Mean of tensor along columns is : {torch.mean(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# mean along rows\n",
    "print(f\"Mean of tensor along rows is : {torch.mean(e, dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Median of tensor is : 8.0\n",
      "\n",
      "Median of tensor along columns is : torch.return_types.median(\n",
      "values=tensor([5., 7., 8.], dtype=torch.float64),\n",
      "indices=tensor([0, 1, 0]))\n",
      "\n",
      "Median of tensor along rows is : torch.return_types.median(\n",
      "values=tensor([8., 9.], dtype=torch.float64),\n",
      "indices=tensor([2, 0]))\n"
     ]
    }
   ],
   "source": [
    "# Median\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Median of tensor is : {torch.median(e)}\")\n",
    "print()\n",
    "\n",
    "# Median along columns\n",
    "print(f\"Median of tensor along columns is : {torch.median(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# Median along rows\n",
    "print(f\"Median of tensor along rows is : {torch.median(e, dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Maximum of tensor is : 9.0\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum of tensor along columns is : torch.return_types.max(\n",
      "values=tensor([9., 9., 9.], dtype=torch.float64),\n",
      "indices=tensor([1, 0, 1]))\n",
      "\n",
      "Maximum of tensor along rows is : torch.return_types.max(\n",
      "values=tensor([9., 9.], dtype=torch.float64),\n",
      "indices=tensor([1, 0]))\n"
     ]
    }
   ],
   "source": [
    "# max \n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Maximum of tensor is : {torch.max(e)}\")\n",
    "print()\n",
    "\n",
    "# max along columns\n",
    "print(f\"Maximum of tensor along columns is : {torch.max(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# max along rows\n",
    "print(f\"Maximum of tensor along rows is : {torch.max(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "30a5eff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Minimum of tensor is : 5.0\n",
      "\n",
      "Minimum of tensor along columns is : torch.return_types.min(\n",
      "values=tensor([5., 7., 8.], dtype=torch.float64),\n",
      "indices=tensor([0, 1, 0]))\n",
      "\n",
      "Minimum of tensor along rows is : torch.return_types.min(\n",
      "values=tensor([5., 7.], dtype=torch.float64),\n",
      "indices=tensor([0, 1]))\n"
     ]
    }
   ],
   "source": [
    "# min \n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Minimum of tensor is : {torch.min(e)}\")\n",
    "print()\n",
    "\n",
    "# min along columns\n",
    "print(f\"Minimum of tensor along columns is : {torch.min(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# min along rows\n",
    "print(f\"Minimum of tensor along rows is : {torch.min(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c4ddd7aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Product of tensor is : 204120.0\n",
      "\n",
      "Product of tensor along columns is : tensor([45., 63., 72.], dtype=torch.float64)\n",
      "\n",
      "Product of tensor along rows is : tensor([360., 567.], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# product (prod) \n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Product of tensor is : {torch.prod(e)}\")\n",
    "print()\n",
    "\n",
    "# product along columns\n",
    "print(f\"Product of tensor along columns is : {torch.prod(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# product along rows\n",
    "print(f\"Product of tensor along rows is : {torch.prod(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Standaed deviation of tensor is : 1.6020819787597222\n",
      "\n",
      "Standaed deviation of tensor along columns is : tensor([2.8284, 1.4142, 0.7071], dtype=torch.float64)\n",
      "\n",
      "Standaed deviation of tensor along rows is : tensor([2.0817, 1.1547], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Standaed deviation\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Standaed deviation of tensor is : {torch.std(e)}\")\n",
    "print()\n",
    "\n",
    "# Standaed deviation along columns\n",
    "print(f\"Standaed deviation of tensor along columns is : {torch.std(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# Standaed deviation along rows\n",
    "print(f\"Standaed deviation of tensor along rows is : {torch.std(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d421e596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Variance of tensor is : 2.5666666666666673\n",
      "\n",
      "Variance of tensor along columns is : tensor([8.0000, 2.0000, 0.5000], dtype=torch.float64)\n",
      "\n",
      "Variance of tensor along rows is : tensor([4.3333, 1.3333], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Variance\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Variance of tensor is : {torch.var(e)}\")\n",
    "print()\n",
    "\n",
    "# Variance along columns\n",
    "print(f\"Variance of tensor along columns is : {torch.var(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# Variance along rows\n",
    "print(f\"Variance of tensor along rows is : {torch.var(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance of tensor is : 1\n",
      "\n",
      "argmax of tensor along columns is : tensor([1, 0, 1])\n",
      "\n",
      "argmax of tensor along rows is : tensor([1, 0])\n"
     ]
    }
   ],
   "source": [
    "# argmax - give position of the largest number\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Variance of tensor is : {torch.argmax(e)}\")\n",
    "print()\n",
    "\n",
    "# argmax along columns\n",
    "print(f\"argmax of tensor along columns is : {torch.argmax(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# argmax along rows\n",
    "print(f\"argmax of tensor along rows is : {torch.argmax(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a44465ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5., 9., 8.],\n",
      "        [9., 7., 9.]], dtype=torch.float64)\n",
      "\n",
      "Variance of tensor is : 0\n",
      "\n",
      "argmax of tensor along columns is : tensor([0, 1, 0])\n",
      "\n",
      "argmax of tensor along rows is : tensor([0, 1])\n"
     ]
    }
   ],
   "source": [
    "# argmin - give position of the largest number\n",
    "print(e)\n",
    "print()\n",
    "\n",
    "print(f\"Variance of tensor is : {torch.argmin(e)}\")\n",
    "print()\n",
    "\n",
    "# argmin along columns\n",
    "print(f\"argmax of tensor along columns is : {torch.argmin(e, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# argmin along rows\n",
    "print(f\"argmax of tensor along rows is : {torch.argmin(e, dim=1)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300a3e5f",
   "metadata": {},
   "source": [
    "### 4. Matrix Operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29545b9f",
   "metadata": {},
   "source": [
    "# ðŸ§® Matrix Operations in PyTorch (Research-level Guide)\n",
    "\n",
    "PyTorch provides powerful and efficient APIs for matrix operations, supporting high-performance computation on both CPU and GPU.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Matrix Creation\n",
    "\n",
    "```\n",
    "A = torch.tensor([[1., 2.], [3., 4.]])\n",
    "B = torch.tensor([[5., 6.], [7., 8.]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Basic Matrix Arithmetic\n",
    "\n",
    "| Operation        | Function / Syntax   | Description                  |\n",
    "|------------------|---------------------|------------------------------|\n",
    "| Addition         | `A + B` or `torch.add(A, B)` | Element-wise addition       |\n",
    "| Subtraction      | `A - B`             | Element-wise subtraction     |\n",
    "| Scalar Multiplication | `A * 2`        | Each element multiplied by scalar |\n",
    "| Element-wise Multiplication | `A * B`  | Element-wise product         |\n",
    "| Division         | `A / B`             | Element-wise division        |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Matrix Multiplication\n",
    "\n",
    "### ðŸŸ  Matrix Product (Dot Product)\n",
    "\n",
    "```\n",
    "torch.matmul(A, B)\n",
    "A @ B  # Equivalent\n",
    "```\n",
    "\n",
    "- Performs matrix multiplication.\n",
    "- Requires shape compatibility: `(m x n) @ (n x p) â†’ (m x p)`\n",
    "\n",
    "### ðŸŸ  Batch Matrix Multiplication\n",
    "\n",
    "```\n",
    "torch.bmm(A, B)  # For 3D tensors (batch_size, n, m)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Transpose and Reshape\n",
    "\n",
    "| Operation      | Function                    |\n",
    "|----------------|-----------------------------|\n",
    "| Transpose      | `A.T` or `torch.transpose(A, 0, 1)` |\n",
    "| Reshape        | `A.view(new_shape)`         |\n",
    "| Permute dims   | `A.permute(dim1, dim2, ...)`|\n",
    "| Squeeze        | `A.squeeze()`               |\n",
    "| Unsqueeze      | `A.unsqueeze(dim)`          |\n",
    "\n",
    "```\n",
    "A.T  # Transpose of A\n",
    "A.view(4)  # Reshape to 1D\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Determinant, Inverse & Rank\n",
    "\n",
    "| Operation        | Function                |\n",
    "|------------------|-------------------------|\n",
    "| Determinant      | `torch.linalg.det(A)`   |\n",
    "| Inverse          | `torch.linalg.inv(A)`   |\n",
    "| Matrix Rank      | `torch.linalg.matrix_rank(A)` |\n",
    "| Pseudo-inverse   | `torch.linalg.pinv(A)`  |\n",
    "\n",
    "```\n",
    "torch.linalg.inv(A)  # Inverse of matrix A\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. Eigenvalues, Eigenvectors & Decompositions\n",
    "\n",
    "| Operation           | Function                    |\n",
    "|---------------------|-----------------------------|\n",
    "| Eigenvalues/Vectors | `torch.linalg.eig(A)`       |\n",
    "| SVD (Singular Value Decomposition) | `torch.linalg.svd(A)` |\n",
    "| QR Decomposition     | `torch.linalg.qr(A)`        |\n",
    "| Cholesky (if SPD)    | `torch.linalg.cholesky(A)`  |\n",
    "\n",
    "```\n",
    "eigenvalues, eigenvectors = torch.linalg.eig(A)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 7. Special Matrix Ops\n",
    "\n",
    "| Operation          | Function                    |\n",
    "|--------------------|-----------------------------|\n",
    "| Identity Matrix     | `torch.eye(n)`             |\n",
    "| Diagonal Matrix     | `torch.diag(input)`        |\n",
    "| Trace (sum of diag) | `torch.trace(A)`           |\n",
    "| Outer Product       | `torch.ger(a, b)` or `a.unsqueeze(1) @ b.unsqueeze(0)` |\n",
    "| Kronecker Product   | `torch.kron(A, B)`         |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 8. Broadcasting in Matrix Ops\n",
    "\n",
    "PyTorch supports **broadcasting** similar to NumPy:\n",
    "```\n",
    "A = torch.randn(3, 1)\n",
    "B = torch.randn(1, 4)\n",
    "C = A * B  # Result: shape (3, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 9. Matrix Norms\n",
    "\n",
    "| Operation       | Function                        |\n",
    "|-----------------|----------------------------------|\n",
    "| Frobenius Norm  | `torch.norm(A)`                 |\n",
    "| p-Norm          | `torch.norm(A, p=1/2/âˆž)`         |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "| Task                  | PyTorch Function            |\n",
    "|-----------------------|-----------------------------|\n",
    "| Matrix Multiply       | `torch.matmul`, `@`         |\n",
    "| Transpose             | `.T`                        |\n",
    "| Inverse               | `torch.linalg.inv`          |\n",
    "| Determinant           | `torch.linalg.det`          |\n",
    "| SVD                   | `torch.linalg.svd`          |\n",
    "| Eigen decomposition   | `torch.linalg.eig`          |\n",
    "| Identity Matrix       | `torch.eye`                 |\n",
    "| Diagonal              | `torch.diag`                |\n",
    "| Trace                 | `torch.trace`               |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… PyTorchâ€™s matrix APIs are optimized with BLAS/LAPACK backend, and can be accelerated on CUDA (GPU) for large-scale computations.\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: tensor([[2, 6, 7],\n",
      "        [7, 8, 3]])\n",
      "\n",
      "g: tensor([[6, 1],\n",
      "        [5, 5],\n",
      "        [0, 4]])\n"
     ]
    }
   ],
   "source": [
    "f = torch.randint(size=(2, 3), low=0, high=10)\n",
    "g = torch.randint(size=(3, 2), low=0, high=10)\n",
    "\n",
    "print(f\"f: {f}\")\n",
    "print()\n",
    "print(f\"g: {g}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1e9f2cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[42, 60],\n",
       "        [82, 59]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# matrix multiplication\n",
    "torch.matmul(f, g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector1: tensor([1, 2])\n",
      "\n",
      "vector2: tensor([3, 4])\n"
     ]
    }
   ],
   "source": [
    "vector1 = torch.tensor([1, 2])\n",
    "vector2 = torch.tensor([3, 4])\n",
    "\n",
    "print(f\"vector1: {vector1}\")\n",
    "print()\n",
    "print(f\"vector2: {vector2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "21892ec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "torch.dot(vector1, vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f: tensor([[2, 6, 7],\n",
      "        [7, 8, 3]])\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[2, 7],\n",
       "        [6, 8],\n",
       "        [7, 3]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transpose\n",
    "print(f\"f: {f}\")\n",
    "print()\n",
    "\n",
    "torch.transpose(f, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fc429a4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3., 8., 8.],\n",
       "        [3., 3., 5.],\n",
       "        [0., 6., 4.]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.randint(size=(3, 3), low=0, high=10, dtype=torch.float32)\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ee7fe0e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-6.)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# determinant\n",
    "torch.det(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 3.0000, -2.6667, -2.6667],\n",
       "        [ 2.0000, -2.0000, -1.5000],\n",
       "        [-3.0000,  3.0000,  2.5000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inverse\n",
    "torch.inverse(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78995a16",
   "metadata": {},
   "source": [
    "### 5. Comparison Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f16f8f",
   "metadata": {},
   "source": [
    "# ðŸ” Comparison Operations in PyTorch\n",
    "\n",
    "PyTorch supports a wide variety of **element-wise** comparison operations. These are essential for tasks like masking, conditional logic, filtering tensors, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Element-wise Comparison Operators\n",
    "\n",
    "| Operation               | Syntax / Function                  | Description                         |\n",
    "|-------------------------|------------------------------------|-------------------------------------|\n",
    "| Equal                   | `A == B` or `torch.eq(A, B)`       | Checks if elements of A and B are equal |\n",
    "| Not Equal               | `A != B` or `torch.ne(A, B)`       | Checks if elements are not equal     |\n",
    "| Greater Than            | `A > B` or `torch.gt(A, B)`        | Checks if A elements are greater than B |\n",
    "| Greater Than or Equal   | `A >= B` or `torch.ge(A, B)`       | Checks if A elements are â‰¥ B         |\n",
    "| Less Than               | `A < B` or `torch.lt(A, B)`        | Checks if A elements are < B         |\n",
    "| Less Than or Equal      | `A <= B` or `torch.le(A, B)`       | Checks if A elements are â‰¤ B         |\n",
    "\n",
    "### ðŸ§ª Example:\n",
    "\n",
    "```\n",
    "import torch\n",
    "\n",
    "A = torch.tensor([1, 2, 3])\n",
    "B = torch.tensor([2, 2, 1])\n",
    "\n",
    "A == B  # tensor([False, True, False])\n",
    "A > B   # tensor([False, False, True])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Logical Comparison on Boolean Tensors\n",
    "\n",
    "| Operation         | Syntax / Function            | Description                          |\n",
    "|-------------------|------------------------------|--------------------------------------|\n",
    "| Logical AND       | `torch.logical_and(A, B)`    | Element-wise AND of boolean tensors |\n",
    "| Logical OR        | `torch.logical_or(A, B)`     | Element-wise OR                      |\n",
    "| Logical NOT       | `torch.logical_not(A)`       | Inverts boolean tensor               |\n",
    "| Logical XOR       | `torch.logical_xor(A, B)`    | Element-wise XOR                     |\n",
    "\n",
    "```\n",
    "A = torch.tensor([True, False, True])\n",
    "B = torch.tensor([False, False, True])\n",
    "\n",
    "torch.logical_and(A, B)  # tensor([False, False, True])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Functions Returning Boolean Summary\n",
    "\n",
    "| Function               | Description                                      |\n",
    "|------------------------|--------------------------------------------------|\n",
    "| `torch.all(tensor)`    | True if **all** elements are True                |\n",
    "| `torch.any(tensor)`    | True if **any** element is True                  |\n",
    "| `torch.isclose(A, B)`  | True if elements of A and B are close (within tolerance) |\n",
    "| `torch.equal(A, B)`    | True if **all elements and shapes** are equal   |\n",
    "| `torch.allclose(A, B)` | True if all elements are close within tolerance |\n",
    "\n",
    "```\n",
    "torch.all(torch.tensor([True, True]))       # True\n",
    "torch.any(torch.tensor([False, True]))      # True\n",
    "torch.isclose(torch.tensor([1.0]), torch.tensor([1.0001]), rtol=1e-3)  # True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Masking with Comparison Ops\n",
    "\n",
    "You can use comparison results to mask tensors:\n",
    "\n",
    "```\n",
    "A = torch.tensor([10, 20, 30, 40])\n",
    "mask = A > 25\n",
    "filtered = A[mask]  # tensor([30, 40])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Type of Result\n",
    "\n",
    "- All comparison operations return a `torch.bool` tensor of the same shape.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. Note on Broadcasting\n",
    "\n",
    "All comparison operations **support broadcasting** just like arithmetic operations:\n",
    "\n",
    "```\n",
    "A = torch.tensor([[1], [2], [3]])  # Shape (3,1)\n",
    "B = torch.tensor([2, 2, 2])        # Shape (3,)\n",
    "\n",
    "result = A > B  # Broadcasts and compares\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary Table\n",
    "\n",
    "| Operation   | Function          |\n",
    "|-------------|-------------------|\n",
    "| Equal       | `==`, `torch.eq`  |\n",
    "| Not Equal   | `!=`, `torch.ne`  |\n",
    "| Greater     | `>`,  `torch.gt`  |\n",
    "| Less        | `<`,  `torch.lt`  |\n",
    "| Greater Eq  | `>=`, `torch.ge`  |\n",
    "| Less Eq     | `<=`, `torch.le`  |\n",
    "| Logical AND | `torch.logical_and` |\n",
    "| Logical OR  | `torch.logical_or` |\n",
    "| All True?   | `torch.all`       |\n",
    "| Any True?   | `torch.any`       |\n",
    "\n",
    "---\n",
    "\n",
    "> âœ… Use comparison ops for filtering, condition-based updates, classification thresholds, and tensor validations.\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "021cdfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i : tensor([[0, 8, 4],\n",
      "        [7, 2, 3]])\n",
      "\n",
      "j : tensor([[8, 5, 6],\n",
      "        [2, 9, 5]])\n"
     ]
    }
   ],
   "source": [
    "i = torch.randint(size=(2, 3), low=0, high=10)\n",
    "j = torch.randint(size=(2, 3), low=0, high=10)\n",
    "\n",
    "print(f\"i : {i}\")\n",
    "print()\n",
    "print(f\"j : {j}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9df48762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i > j : tensor([[False,  True, False],\n",
      "        [ True, False, False]])\n",
      "\n",
      "i < j : tensor([[ True, False,  True],\n",
      "        [False,  True,  True]])\n",
      "\n",
      "i >= j : tensor([[False,  True, False],\n",
      "        [ True, False, False]])\n",
      "\n",
      "i <= j : tensor([[ True, False,  True],\n",
      "        [False,  True,  True]])\n",
      "\n",
      "i == j : tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "\n",
      "i != j : tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# greater\n",
    "print(f\"i > j : {i > j}\")\n",
    "print()\n",
    "\n",
    "# less\n",
    "print(f\"i < j : {i < j}\")\n",
    "print()\n",
    "\n",
    "# greater equal\n",
    "print(f\"i >= j : {i >= j}\")\n",
    "print()\n",
    "\n",
    "# less equal\n",
    "print(f\"i <= j : {i <= j}\")\n",
    "print()\n",
    "\n",
    "# equal\n",
    "print(f\"i == j : {i == j}\")\n",
    "print()\n",
    "\n",
    "# not equal\n",
    "print(f\"i != j : {i != j}\")\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81579d4",
   "metadata": {},
   "source": [
    "### 6. Special Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390bf1b8",
   "metadata": {},
   "source": [
    "# ðŸŒŸ Special Functions in PyTorch (`torch.special`)\n",
    "\n",
    "The `torch.special` module provides **numerically stable**, **scientifically useful** mathematical functions used widely in **scientific computing, statistics, deep learning, and physics**.\n",
    "\n",
    "These functions include **gamma functions**, **error functions**, **log-sum-exp**, **Bessel functions**, and more.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 1. Gamma and Related Functions\n",
    "\n",
    "| Function                  | Description                                          |\n",
    "|---------------------------|------------------------------------------------------|\n",
    "| `torch.special.gamma(x)` | Computes the Gamma function Î“(x)                     |\n",
    "| `torch.special.gammaln(x)` | Computes `log(abs(gamma(x)))`                       |\n",
    "| `torch.special.digamma(x)` | First derivative of `log(gamma(x))`                |\n",
    "| `torch.special.polygamma(n, x)` | n-th derivative of `log(gamma(x))`           |\n",
    "\n",
    "ðŸ“Œ **Use Case**: Often used in probabilistic models and distributions.\n",
    "\n",
    "```\n",
    "import torch\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "torch.special.gamma(x)     # tensor([1., 1., 2.])\n",
    "torch.special.digamma(x)   # tensor([-0.5772, 0.4228, 0.9228])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 2. Error Functions\n",
    "\n",
    "| Function                  | Description                           |\n",
    "|---------------------------|---------------------------------------|\n",
    "| `torch.special.erf(x)`   | Error function                         |\n",
    "| `torch.special.erfc(x)`  | Complementary error function (1 - erf) |\n",
    "| `torch.special.erfinv(x)` | Inverse error function                |\n",
    "\n",
    "ðŸ“Œ **Use Case**: Used in Gaussian distributions, signal processing, and diffusion models.\n",
    "\n",
    "```\n",
    "torch.special.erf(torch.tensor(1.0))    # â‰ˆ 0.8427\n",
    "torch.special.erfinv(torch.tensor(0.8427))  # â‰ˆ 1.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 3. Exponential Logarithmic Stability\n",
    "\n",
    "| Function                         | Description                                      |\n",
    "|----------------------------------|--------------------------------------------------|\n",
    "| `torch.special.expit(x)`        | Sigmoid function: 1 / (1 + exp(-x))              |\n",
    "| `torch.special.logit(x)`        | Inverse of sigmoid (log(x / (1 - x)))            |\n",
    "| `torch.special.logsumexp(x, dim)` | Stable `log(sum(exp(x)))`                       |\n",
    "\n",
    "ðŸ“Œ **Use Case**: Useful in classification, loss computation (e.g., softmax-related ops).\n",
    "\n",
    "```\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "torch.special.logsumexp(x, dim=0)  # â‰ˆ log(exp(1)+exp(2)+exp(3))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 4. Bessel Functions\n",
    "\n",
    "| Function                          | Description                         |\n",
    "|-----------------------------------|-------------------------------------|\n",
    "| `torch.special.i0(x)`            | Modified Bessel function of the first kind (order 0) |\n",
    "| `torch.special.i0e(x)`           | Scaled modified Bessel function     |\n",
    "\n",
    "ðŸ“Œ **Use Case**: Appears in wave equations, signal processing.\n",
    "\n",
    "```\n",
    "x = torch.tensor([0.0, 1.0, 2.0])\n",
    "torch.special.i0(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 5. Softmax Variants\n",
    "\n",
    "| Function                    | Description                                |\n",
    "|-----------------------------|--------------------------------------------|\n",
    "| `torch.special.softmax(x, dim)` | Softmax function                       |\n",
    "| `torch.special.log_softmax(x, dim)` | Log-Softmax function                |\n",
    "\n",
    "ðŸ“Œ **Use Case**: Classification problems, attention mechanisms, etc.\n",
    "\n",
    "```\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "torch.special.softmax(x, dim=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ 6. Other Special Functions\n",
    "\n",
    "| Function                         | Description                             |\n",
    "|----------------------------------|-----------------------------------------|\n",
    "| `torch.special.ndtr(x)`         | Standard normal cumulative distribution |\n",
    "| `torch.special.ndtri(x)`        | Inverse of standard normal CDF          |\n",
    "| `torch.special.multigammaln(x, d)` | Log multivariate gamma function        |\n",
    "| `torch.special.xlog1py(x, y)`   | Computes `x * log1p(y)` safely          |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Summary Table\n",
    "\n",
    "| Category              | Functions                                                                 |\n",
    "|------------------------|--------------------------------------------------------------------------|\n",
    "| Gamma Family          | `gamma`, `gammaln`, `digamma`, `polygamma`, `multigammaln`               |\n",
    "| Error Functions       | `erf`, `erfc`, `erfinv`                                                   |\n",
    "| Log/Exp Stability     | `expit`, `logit`, `logsumexp`, `xlog1py`                                  |\n",
    "| Softmax Variants      | `softmax`, `log_softmax`                                                  |\n",
    "| Bessel Functions      | `i0`, `i0e`                                                                |\n",
    "| Gaussian/Normal Dist. | `ndtr`, `ndtri`                                                           |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Use in Research\n",
    "\n",
    "- ðŸ§¬ **Probabilistic Modeling**: Gamma, digamma, log-gamma functions.\n",
    "- ðŸ“ˆ **Machine Learning**: Softmax, log-sum-exp, expit/logit for classification & optimization.\n",
    "- ðŸ“Š **Statistical Analysis**: ndtr, ndtri for normal distribution approximations.\n",
    "- ðŸ§  **Numerical Stability**: log1p, expit, logsumexp help prevent underflow/overflow.\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ” For complete list and doc: [PyTorch Special Functions Docs](https://pytorch.org/docs/stable/special.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 4, 2],\n",
       "        [7, 1, 1]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randint(size=(2, 3), low=0, high=10)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  -inf, 1.3863, 0.6931],\n",
       "        [1.9459, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# log\n",
    "torch.log(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000e+00, 5.4598e+01, 7.3891e+00],\n",
       "        [1.0966e+03, 2.7183e+00, 2.7183e+00]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exp\n",
    "torch.exp(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 2.0000, 1.4142],\n",
       "        [2.6458, 1.0000, 1.0000]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sqrt\n",
    "torch.sqrt(k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3d144687",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000, 0.9820, 0.8808],\n",
       "        [0.9991, 0.7311, 0.7311]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sigmoid\n",
    "torch.sigmoid(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fef99432",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 4., 4.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = torch.randint(size=(2, 3), low=0, high=10, dtype=torch.float32)\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "742b9c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax along columns : tensor([[0.9820, 0.9526, 0.8808],\n",
      "        [0.0180, 0.0474, 0.1192]])\n",
      "\n",
      "Softmax along rows : tensor([[0.5761, 0.2119, 0.2119],\n",
      "        [0.2119, 0.2119, 0.5761]])\n"
     ]
    }
   ],
   "source": [
    "# softmax is not implemented for int 64, int 32, int 16, int 8\n",
    "\n",
    "# dim=0 means along columns\n",
    "# dim=1 means along rows\n",
    "\n",
    "# softmax along columns\n",
    "print(f\"Softmax along columns : {torch.softmax(k, dim=0)}\")\n",
    "print()\n",
    "\n",
    "# softmax along rows\n",
    "print(f\"Softmax along rows : {torch.softmax(k, dim=1)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "19c11c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 4., 4.],\n",
       "        [1., 1., 2.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# relu\n",
    "torch.relu(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ab8c3",
   "metadata": {},
   "source": [
    "# Inplace Operations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a181",
   "metadata": {},
   "source": [
    "# ðŸ”„ In-place Operations in PyTorch\n",
    "\n",
    "In PyTorch, **in-place operations** directly modify the content of a tensor without making a copy. They are **memory-efficient** but can **interfere with autograd** (automatic differentiation) if not used carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ How to Identify In-place Operations?\n",
    "\n",
    "In PyTorch, **in-place methods** end with an exclamation mark (`!`).\n",
    "\n",
    "> âœ… Example: `tensor.add_(value)` is an in-place version of `tensor.add(value)`\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Common In-place Operations\n",
    "\n",
    "| Operation             | In-place Version       | Description                                 |\n",
    "|-----------------------|------------------------|---------------------------------------------|\n",
    "| Addition              | `tensor.add_(x)`       | Adds `x` to tensor                          |\n",
    "| Subtraction           | `tensor.sub_(x)`       | Subtracts `x` from tensor                   |\n",
    "| Multiplication        | `tensor.mul_(x)`       | Multiplies tensor by `x`                    |\n",
    "| Division              | `tensor.div_(x)`       | Divides tensor by `x`                       |\n",
    "| Power                 | `tensor.pow_(x)`       | Raises tensor to power `x`                  |\n",
    "| Square Root           | `tensor.sqrt_()`       | Computes square root                        |\n",
    "| Exponential           | `tensor.exp_()`        | Element-wise exponential                    |\n",
    "| Logarithm             | `tensor.log_()`        | Element-wise log                            |\n",
    "| Floor                 | `tensor.floor_()`      | Applies floor operation                     |\n",
    "| Ceil                  | `tensor.ceil_()`       | Applies ceil operation                      |\n",
    "| Clamp                 | `tensor.clamp_(min, max)` | Clamps values within range              |\n",
    "| Zeroing              | `tensor.zero_()`       | Sets all values to zero                     |\n",
    "| Fill                 | `tensor.fill_(x)`      | Fills tensor with value `x`                 |\n",
    "| Copy                 | `tensor.copy_(other)`  | Copies content from `other` tensor          |\n",
    "| Normalization        | `tensor.normal_()`     | Fills with samples from normal distribution |\n",
    "| Uniform              | `tensor.uniform_()`    | Fills with uniform distribution             |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Example Code\n",
    "\n",
    "```\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(\"Before:\", x)\n",
    "\n",
    "x.add_(5)\n",
    "print(\"After In-place Add:\", x)\n",
    "\n",
    "x.mul_(2)\n",
    "print(\"After In-place Multiply:\", x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Warning: Use with Caution in Autograd\n",
    "\n",
    "In-place operations can **overwrite values needed for gradient computation**, causing errors in backpropagation.\n",
    "\n",
    "```\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "\n",
    "y = x * x\n",
    "y.backward()  # works fine\n",
    "\n",
    "x.add_(1)     # modifies x in-place â€” risky for autograd\n",
    "```\n",
    "\n",
    "> âš ï¸ Prefer out-of-place operations during training unless you're sure of their impact.\n",
    "\n",
    "---\n",
    "\n",
    "## âœ… When to Use In-place Ops?\n",
    "\n",
    "- When memory is limited (e.g., training large models).\n",
    "- During inference or data preprocessing.\n",
    "- When gradients are not required (`requires_grad=False`).\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Tips\n",
    "\n",
    "- All `tensor.function_()` operations are in-place.\n",
    "- Use `.clone()` if you want to preserve original tensor.\n",
    "- Avoid using them during model training unless safe.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "| Safe For Training? | Use-case                         |\n",
    "|--------------------|----------------------------------|\n",
    "| âœ…                 | Preprocessing, inference         |\n",
    "| âŒ (with autograd) | During backpropagation/training  |\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ” For more: [PyTorch In-Place Docs](https://pytorch.org/docs/stable/notes/autograd.html#in-place-operations)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8fcd1f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m : tensor([[0.9186, 0.2131, 0.3957],\n",
      "        [0.6017, 0.4234, 0.5224]])\n",
      "\n",
      "n : tensor([[0.4175, 0.0340, 0.9157],\n",
      "        [0.3079, 0.6269, 0.8277]])\n"
     ]
    }
   ],
   "source": [
    "m = torch.rand(2, 3)\n",
    "n = torch.rand(2, 3)\n",
    "\n",
    "print(f\"m : {m}\")\n",
    "print()\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93be6428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3361, 0.2472, 1.3114],\n",
       "        [0.9096, 1.0504, 1.3501]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.add_(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "abbd4b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m : tensor([[1.3361, 0.2472, 1.3114],\n",
      "        [0.9096, 1.0504, 1.3501]])\n",
      "\n",
      "n : tensor([[0.4175, 0.0340, 0.9157],\n",
      "        [0.3079, 0.6269, 0.8277]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"m : {m}\")\n",
    "print()\n",
    "print(f\"n : {n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "52bfccc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3361, 0.2472, 1.3114],\n",
       "        [0.9096, 1.0504, 1.3501]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.relu(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3361, 0.2472, 1.3114],\n",
       "        [0.9096, 1.0504, 1.3501]])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.relu_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "88ffe6d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m : tensor([[1.3361, 0.2472, 1.3114],\n",
      "        [0.9096, 1.0504, 1.3501]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"m : {m}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895a51a",
   "metadata": {},
   "source": [
    "# Copying a Tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448afc91",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "45612ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : tensor([[0.6594, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n",
      "\n",
      "b : tensor([[0.6594, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n"
     ]
    }
   ],
   "source": [
    "# Copying using assignment operator\n",
    "a = torch.rand(2, 3)\n",
    "b = a\n",
    "\n",
    "print(f\"a : {a}\")\n",
    "print()\n",
    "print(f\"b : {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : tensor([[0.0000, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n",
      "\n",
      "b : tensor([[0.0000, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a : {a}\")\n",
    "print()\n",
    "print(f\"b : {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9d4f2c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of a : 140047920295488\n",
      "\n",
      "id of b : 140047920295488\n"
     ]
    }
   ],
   "source": [
    "print(f\"id of a : {id(a)}\")\n",
    "print()\n",
    "print(f\"id of b : {id(b)}\")\n",
    "\n",
    "# Assignment operator does not create a new tensor, it just creates a reference to the original tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c947a066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : tensor([[0.0000, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n",
      "\n",
      "b : tensor([[0.0000, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n"
     ]
    }
   ],
   "source": [
    "# coping using clone()\n",
    "b = a.clone()\n",
    "\n",
    "print(f\"a : {a}\")\n",
    "print()\n",
    "print(f\"b : {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0] = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a : tensor([[1.0000e+02, 8.8695e-02, 4.8896e-01],\n",
      "        [5.8873e-01, 7.3401e-01, 8.4972e-01]])\n",
      "\n",
      "b : tensor([[0.0000, 0.0887, 0.4890],\n",
      "        [0.5887, 0.7340, 0.8497]])\n"
     ]
    }
   ],
   "source": [
    "print(f\"a : {a}\")\n",
    "print()\n",
    "print(f\"b : {b}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6e457e94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "id of a : 140047920295488\n",
      "\n",
      "id of b : 140047920337120\n"
     ]
    }
   ],
   "source": [
    "print(f\"id of a : {id(a)}\")\n",
    "print()\n",
    "print(f\"id of b : {id(b)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48904a36",
   "metadata": {},
   "source": [
    "# Tensor Operations on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84ac935",
   "metadata": {},
   "source": [
    "# âš™ï¸ Tensor Operations on GPU in PyTorch\n",
    "\n",
    "## ðŸš€ Introduction\n",
    "\n",
    "PyTorch makes it easy to utilize **GPU acceleration** via CUDA (NVIDIAâ€™s GPU computing toolkit). Using GPU can **significantly boost performance**, especially for deep learning and large matrix computations.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Check GPU Availability\n",
    "\n",
    "```\n",
    "import torch\n",
    "\n",
    "# Check if CUDA (GPU) is available\n",
    "torch.cuda.is_available()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Set Device\n",
    "\n",
    "```\n",
    "# Automatically choose GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Creating Tensors on GPU\n",
    "\n",
    "```\n",
    "# Method 1: Create directly on GPU\n",
    "x = torch.tensor([1.0, 2.0, 3.0], device=device)\n",
    "\n",
    "# Method 2: Create on CPU, then move to GPU\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "x = x.to(device)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Tensor Operations on GPU\n",
    "\n",
    "Once a tensor is on GPU, any operation you perform on it **stays on GPU**:\n",
    "\n",
    "```\n",
    "a = torch.tensor([1.0, 2.0], device=device)\n",
    "b = torch.tensor([3.0, 4.0], device=device)\n",
    "c = a + b  # operation on GPU\n",
    "\n",
    "print(c.device)  # Output: cuda:0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Moving Between Devices\n",
    "\n",
    "```\n",
    "# Move to GPU\n",
    "x_gpu = x.to(\"cuda\")\n",
    "\n",
    "# Move back to CPU\n",
    "x_cpu = x_gpu.to(\"cpu\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Allocate Tensors Like Another (Preserving Device)\n",
    "\n",
    "```\n",
    "a = torch.ones((2, 2), device=device)\n",
    "b = torch.zeros_like(a)  # b will also be on the same device as a\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Model on GPU\n",
    "\n",
    "```\n",
    "import torch.nn as nn\n",
    "\n",
    "model = nn.Linear(10, 5)\n",
    "model.to(device)  # move model to GPU\n",
    "```\n",
    "\n",
    "Then move input and targets:\n",
    "\n",
    "```\n",
    "input = torch.randn(2, 10).to(device)\n",
    "output = model(input)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”¹ Performance Comparison (Optional Benchmarking)\n",
    "\n",
    "```\n",
    "import time\n",
    "\n",
    "cpu_tensor = torch.randn(10000, 10000)\n",
    "gpu_tensor = torch.randn(10000, 10000, device='cuda')\n",
    "\n",
    "# CPU timing\n",
    "start = time.time()\n",
    "_ = cpu_tensor @ cpu_tensor\n",
    "print(\"CPU Time:\", time.time() - start)\n",
    "\n",
    "# GPU timing\n",
    "torch.cuda.synchronize()  # synchronize before timing\n",
    "start = time.time()\n",
    "_ = gpu_tensor @ gpu_tensor\n",
    "torch.cuda.synchronize()\n",
    "print(\"GPU Time:\", time.time() - start)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ Notes and Tips\n",
    "\n",
    "- GPU ops are **asynchronous** by default. Use `torch.cuda.synchronize()` to benchmark accurately.\n",
    "- Always move **both model and data** to the same device.\n",
    "- Avoid frequent device switching â€“ itâ€™s **slow and inefficient**.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Summary\n",
    "\n",
    "| Operation              | Example                            |\n",
    "|------------------------|------------------------------------|\n",
    "| Create on GPU          | `torch.tensor(..., device='cuda')` |\n",
    "| Move to GPU            | `tensor.to(\"cuda\")`                |\n",
    "| Move to CPU            | `tensor.to(\"cpu\")`                 |\n",
    "| Move model to GPU      | `model.to(\"cuda\")`                 |\n",
    "| Check device           | `tensor.device`                    |\n",
    "| Check GPU availability | `torch.cuda.is_available()`        |\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ”— Official Docs: [PyTorch CUDA Semantics](https://pytorch.org/docs/stable/notes/cuda.html)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "aa960835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ca1bd159",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[78]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Creating a tensor on GPU\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m a = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Academics/PyTorch/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# Creating a tensor on GPU\n",
    "a = torch.rand((2, 3), device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9112, 0.4847, 0.9436],\n",
       "        [0.3904, 0.2499, 0.3206]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moving a tensor to GPU\n",
    "a = torch.rand(2,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0f347291",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[80]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m b = \u001b[43ma\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Academics/PyTorch/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "b = a.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.0000, 5.0887, 5.4890],\n",
       "        [5.5887, 5.7340, 5.8497]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b + 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "571a508a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken on CPU: 30.9514 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[82]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTime taken on CPU: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcpu_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Transferring tenosrs to GPU\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m a_gpu = \u001b[43ma_cpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m b_gpu = b_cpu.to(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Measuring time on GPU\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/d/Academics/PyTorch/venv/lib/python3.12/site-packages/torch/cuda/__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "# comparing CPU and GPU \n",
    "\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Defining the size of the tensor\n",
    "size = 10000 # large size for performance comparison\n",
    "\n",
    "# Creating a tensor on CPU using rand\n",
    "a_cpu = torch.rand(size, size)\n",
    "b_cpu = torch.rand(size, size)\n",
    "\n",
    "# Measuring time on CPU\n",
    "start_time_cpu = time.time()\n",
    "result_cpu = torch.matmul(a_cpu, b_cpu) # matrix multiplication is performed on cPU\n",
    "cpu_time = time.time() - start_time_cpu\n",
    "\n",
    "print(f\"Time taken on CPU: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# Transferring tenosrs to GPU\n",
    "a_gpu = a_cpu.to('cuda')\n",
    "b_gpu = b_cpu.to('cuda')\n",
    "\n",
    "# Measuring time on GPU\n",
    "start_time_gpu = time.time()\n",
    "result_gpu = torch.matmul(a_gpu, b_gpu)  # matrix multiplication is performed on GPU\n",
    "torch.cuda.synchronize()  # Wait for all kernels in all streams on a GPU to finish\n",
    "gpu_time = time.time() - start_time_gpu\n",
    "\n",
    "print(f\"Time taken on GPU: {cpu_time:.4f} seconds\")\n",
    "\n",
    "# Comparing the results\n",
    "print(\"\\nSpeeduo (CPU time / GPU time): \", cpu_time / gpu_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c936f22",
   "metadata": {},
   "source": [
    "# Reshaping Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0573d3fd",
   "metadata": {},
   "source": [
    "# ðŸ”„ Reshaping Tensors in PyTorch\n",
    "\n",
    "Reshaping operations in PyTorch are used to change the shape of a tensor **without changing its data**. These are essential in preprocessing, model input formatting, and data manipulation in deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“Œ Common Reshaping Operations\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.view()`\n",
    "\n",
    "- Returns a new tensor with the same data but different shape.\n",
    "- Only works on **contiguous** tensors.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = x.view(6, 4)  # reshapes to (6, 4)\n",
    "```\n",
    "\n",
    "> âš ï¸ `view()` may fail if the tensor is not contiguous. Use `tensor.contiguous()` before applying it if needed.\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.reshape()`\n",
    "\n",
    "- Similar to `view()` but more flexible.\n",
    "- Can handle non-contiguous tensors.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "y = x.reshape(6, 4)  # reshapes to (6, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.flatten()`\n",
    "\n",
    "- Flattens a multi-dimensional tensor into a 1D tensor.\n",
    "- You can also flatten only part of the dimensions.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "x_flat = x.flatten()        # shape: (24,)\n",
    "x_flat_1 = x.flatten(start_dim=1)  # shape: (2, 12)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.unsqueeze(dim)`\n",
    "\n",
    "- Adds a dimension of size `1` at the specified position.\n",
    "\n",
    "```\n",
    "x = torch.tensor([1.0, 2.0, 3.0])  # shape: (3,)\n",
    "x_unsq = x.unsqueeze(0)           # shape: (1, 3)\n",
    "x_unsq2 = x.unsqueeze(1)          # shape: (3, 1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.squeeze(dim)`\n",
    "\n",
    "- Removes all dimensions of size `1`.\n",
    "\n",
    "```\n",
    "x = torch.randn(1, 3, 1, 5)\n",
    "x_squeezed = x.squeeze()     # removes all dimensions of size 1: shape (3, 5)\n",
    "x_squeezed_1 = x.squeeze(0)  # removes dim 0 if it is 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.permute(dims)`\n",
    "\n",
    "- Permutes (reorders) dimensions of a tensor.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "x_perm = x.permute(1, 0, 2)  # shape: (3, 2, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ”¹ `tensor.transpose(dim0, dim1)`\n",
    "\n",
    "- Swaps two dimensions of a tensor.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3)\n",
    "x_t = x.transpose(0, 1)  # shape: (3, 2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§  Shape Inference with `-1`\n",
    "\n",
    "- `-1` lets PyTorch **automatically calculate** the size of that dimension.\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "x_reshaped = x.view(-1, 4)  # shape: (6, 4)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Example Summary\n",
    "\n",
    "```\n",
    "x = torch.randn(2, 3, 4)\n",
    "\n",
    "x.view(6, 4)           # reshape to (6, 4)\n",
    "x.reshape(2, 12)       # reshape to (2, 12)\n",
    "x.flatten()            # flatten to (24,)\n",
    "x.unsqueeze(0)         # add dimension: (1, 2, 3, 4)\n",
    "x.squeeze()            # remove size 1 dims\n",
    "x.permute(2, 0, 1)     # reorder to (4, 2, 3)\n",
    "x.transpose(0, 1)      # swap first two dims\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "> ðŸ”— **Docs Reference**: [PyTorch Tensor Operations](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones(4, 4)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff35f09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reshape\n",
    "# a.reshape(2, 2, 2, 2)\n",
    "a.reshape(2, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7017a906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flatten\n",
    "a.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3084, 0.2497, 0.5905, 0.9444],\n",
       "         [0.9316, 0.0869, 0.5337, 0.6454],\n",
       "         [0.2783, 0.1392, 0.3548, 0.3154]],\n",
       "\n",
       "        [[0.7155, 0.8725, 0.3784, 0.9932],\n",
       "         [0.8321, 0.4904, 0.5448, 0.8393],\n",
       "         [0.9212, 0.6474, 0.4833, 0.6609]]])"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.rand(2, 3, 4)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 3])"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# permute\n",
    "b.permute(2, 0, 1).shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd6e9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.3084, 0.2497, 0.5905, 0.9444],\n",
       "         [0.9316, 0.0869, 0.5337, 0.6454],\n",
       "         [0.2783, 0.1392, 0.3548, 0.3154]],\n",
       "\n",
       "        [[0.7155, 0.8725, 0.3784, 0.9932],\n",
       "         [0.8321, 0.4904, 0.5448, 0.8393],\n",
       "         [0.9212, 0.6474, 0.4833, 0.6609]]])"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbf1298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 3])"
      ]
     },
     "execution_count": 333,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unsqueeze\n",
    "c = torch.rand(256, 256, 3)\n",
    "c.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# squeeze\n",
    "d = torch.rand(1, 20)\n",
    "d.squeeze(0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb74aa",
   "metadata": {},
   "source": [
    "# Numpy and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7abdacb",
   "metadata": {},
   "source": [
    "# ðŸ”„ NumPy Array and PyTorch Tensor Conversion\n",
    "\n",
    "PyTorch integrates seamlessly with NumPy. You can **convert data between NumPy arrays and PyTorch tensors** without copying data (i.e., sharing memory), which is highly efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Convert NumPy Array â†’ PyTorch Tensor\n",
    "\n",
    "### âœ… Using `torch.from_numpy()`\n",
    "\n",
    "```\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "np_array = np.array([1, 2, 3, 4])\n",
    "torch_tensor = torch.from_numpy(np_array)\n",
    "\n",
    "print(torch_tensor)  # tensor([1, 2, 3, 4])\n",
    "```\n",
    "\n",
    "> âš ï¸ The resulting tensor **shares memory** with the NumPy array. If one is modified, the other is affected.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Convert PyTorch Tensor â†’ NumPy Array\n",
    "\n",
    "### âœ… Using `tensor.numpy()`\n",
    "\n",
    "```\n",
    "torch_tensor = torch.tensor([1, 2, 3, 4])\n",
    "np_array = torch_tensor.numpy()\n",
    "\n",
    "print(np_array)  # [1 2 3 4]\n",
    "```\n",
    "\n",
    "> âš ï¸ Works only if the tensor is on **CPU**. Move to CPU using `.cpu()` if needed.\n",
    "\n",
    "```python\n",
    "tensor_on_gpu = torch.tensor([1, 2, 3], device='cuda')\n",
    "np_array = tensor_on_gpu.cpu().numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ” Bidirectional Conversion & Shared Memory\n",
    "\n",
    "```\n",
    "# Shared memory example\n",
    "arr = np.array([10, 20])\n",
    "ten = torch.from_numpy(arr)\n",
    "arr[0] = 99\n",
    "print(ten)  # tensor([99, 20])\n",
    "```\n",
    "\n",
    "> ðŸ§  These conversions are **zero-copy** (no data duplication), unless explicitly told otherwise.\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§ª Summary Table\n",
    "\n",
    "| Direction              | Function             | Shared Memory? | Notes                            |\n",
    "|------------------------|----------------------|----------------|----------------------------------|\n",
    "| NumPy â†’ Tensor         | `torch.from_numpy()` | âœ… Yes         | Must be contiguous               |\n",
    "| Tensor â†’ NumPy         | `tensor.numpy()`     | âœ… Yes         | Only on CPU tensors              |\n",
    "| GPU Tensor â†’ NumPy     | `.cpu().numpy()`     | âŒ No          | Data is copied to CPU            |\n",
    "| Tensor â†’ NumPy (copy)  | `tensor.detach().cpu().numpy()` | âŒ No | Safe for tensors that require grad |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ”— References\n",
    "\n",
    "- [PyTorch NumPy Bridge Docs](https://pytorch.org/tutorials/beginner/blitz/tensor_tutorial.html#bridge-between-numpy-and-torch-tensor)\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d6de715",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.5'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a4d9df8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# b = a.cpu().numpy()  # convert tensor to numpy array if tensor is on GPU\n",
    "b = a.numpy() # convert tensor to numpy array if tensor is on CPU\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = np.array([1, 2, 3])\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.from_numpy(c)  # convert numpy array to tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
